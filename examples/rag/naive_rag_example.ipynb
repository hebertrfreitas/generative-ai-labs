{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa674d609c08c708",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Naive RAG example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f330093e",
   "metadata": {},
   "source": [
    "\n",
    "To run this example you need to have a postgres with pgvector running.\n",
    "There is a docker-compose file in the root directory, so you can simply do:\n",
    "\n",
    "```bash\n",
    "docker compose up -d \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a888fc19ab43a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T00:57:34.802723Z",
     "start_time": "2025-05-21T00:57:29.898186Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# install required dependencies\n",
    "! pip install --upgrade pip\n",
    "#! pip install langchain_community tiktoken langchain-openai chromadb langchain unstructured \"unstructured[pdf]\" langchain_ollama langchain_postgres \"psycopg2[binary]\"\n",
    "! pip install langchain\n",
    "! pip install langchain_community \n",
    "! pip install langchain-openai \n",
    "#! pip install chromadb\n",
    "! pip install unstructured \"unstructured[pdf]\"\n",
    "#! pip install langchain_ollama \n",
    "! pip install \"psycopg[binary]\"\n",
    "! pip install langchain_postgres \n",
    "! pip install pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c39cef40476a5d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T00:58:11.301540Z",
     "start_time": "2025-05-21T00:58:11.292445Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a596a59317f361a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Getting source documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef7f0fe5189f76f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "!rm -rf 'documents'\n",
    "!mkdir -p 'documents'\n",
    "!curl -L 'https://itau-fn8-fundosdocumentos.cloud.itau.com.br/52678_COMPE.pdf' -o 'documents/52678_COMPE.pdf'\n",
    "!curl -L 'https://itau-fn8-fundosdocumentos.cloud.itau.com.br/55765_COMAG.pdf' -o 'documents/55765_COMAG.pdf'\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65852106681419df",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdb9209148f3814",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "loader = DirectoryLoader(\"./documents\", glob=\"**/*.pdf\")\n",
    "docs = loader.load()\n",
    "len(docs)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b61d16d",
   "metadata": {},
   "source": [
    "## Add metada for filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b058d838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding metadata\n",
    "for doc in splits:\n",
    "    if doc.metadata['source'] == 'documents/55765_COMAG.pdf':\n",
    "        doc.metadata['fundo'] = \"ITAU_FIC_FIM\"\n",
    "    if doc.metadata['source'] == \"documents/52678_COMPE.pdf\":\n",
    "        doc.metadata['fundo'] = \"DIFERENCIADO_CREDITO_PRIVADO_LONGO_PRAZO_RENDA_FIXA\"\n",
    "\n",
    "splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2ae288",
   "metadata": {},
   "source": [
    "## Create database (postgres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4897e584",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import psycopg\n",
    "\n",
    "vector_db_name = \"vector_db_rag\"\n",
    "admin_db_name = \"postgres\"\n",
    "db_host = \"localhost\"\n",
    "db_user = \"postgres\"\n",
    "db_password = \"postgres\"\n",
    "db_port = \"5432\"\n",
    "\n",
    "#connection = \"postgresql+psycopg://langchain:langchain@localhost:6024/langchain\n",
    "\n",
    "connection_string = f\"postgresql+psycopg://{db_user}:{db_password}@{db_host}:{db_port}/{vector_db_name}\"\n",
    "\n",
    "conn = psycopg.connect(dbname=admin_db_name, host=db_host, \n",
    "                       port=db_port, user=db_user, password=db_password)\n",
    "conn.autocommit = True\n",
    "\n",
    "with conn.cursor() as c:\n",
    "    kill_connection_query  = f\"\"\"\n",
    "                                SELECT \n",
    "                                    pg_terminate_backend(pid) \n",
    "                                FROM \n",
    "                                    pg_stat_activity \n",
    "                                WHERE \n",
    "                                    -- don't kill my own connection!\n",
    "                                    pid <> pg_backend_pid()\n",
    "                                    -- don't kill the connections to other databases\n",
    "                                    AND datname = '{vector_db_name}'\n",
    "                                    ;\n",
    "                            \"\"\"\n",
    "    \n",
    "    c.execute(kill_connection_query)\n",
    "\n",
    "with conn.cursor() as c:\n",
    "    c.execute(f\"DROP DATABASE IF EXISTS {vector_db_name}\")\n",
    "    c.execute(f\"CREATE DATABASE {vector_db_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a413783a",
   "metadata": {},
   "source": [
    "## Generating and storing embedding in Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5194d3428284566d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# vector store\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_postgres import PGVector\n",
    "#from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "#from sqlalchemy import make_url    \n",
    "\n",
    "#embedding_model = OllamaEmbeddings(model=\"all-minilm\")\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Embed\n",
    "# vectorstore = Chroma.from_documents(documents=splits, \n",
    "#                                     embedding=embedding_model)\n",
    "\n",
    "vector_store = PGVector(\n",
    "    embeddings=embedding_model,\n",
    "    collection_name=\"fundos_investimento\",\n",
    "    connection=connection_string,\n",
    "    use_jsonb=True,\n",
    ")\n",
    "\n",
    "\n",
    "vector_store.add_documents(documents=splits)\n",
    "\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2a06e4a823e845",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8101b28c9ff7fe2c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# activate debug logging\n",
    "from langchain_core.globals import set_debug, set_verbose\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logging.debug(\"test\")\n",
    "\n",
    "set_debug(True)\n",
    "#set_verbose(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6ecb23",
   "metadata": {},
   "source": [
    "## Answering questions using RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac2e14a812b42f1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# query the vector store\n",
    "#from langchain_ollama import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "# Prompt\n",
    "\n",
    "## filter by metadata\n",
    "retriever = vector_store.as_retriever(search_kwargs={'filter': {'fundo':'ITAU_FIC_FIM'}})\n",
    "\n",
    "# LLM\n",
    "#llm = ChatOllama(model=\"llama3.1\")\n",
    "llm = ChatOpenAI(model=\"gpt-4.1\")\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "     You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. \n",
    "     If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "     Question: {question} \n",
    "     Context: {context} \n",
    "     Answer:\n",
    "     \"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate([(\"system\", system_prompt)])\n",
    "\n",
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt_template\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Question\n",
    "print(rag_chain.invoke(\"Qual o horÃ¡rio limite investir no fundo ?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c724567",
   "metadata": {},
   "source": [
    "---------------\n",
    "### Bonus -> structured output chalenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653fa23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## bonus - enrich query with a structured output\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "class SearchQuery(BaseModel):\n",
    "    \"\"\"Search to execute againts a vector database.\"\"\"\n",
    "\n",
    "    search: str = Field(description=\"The user search\")\n",
    "    fundo_investimento: str = Field(description=\"The investment fund name, \" \\\n",
    "    \"there are only two ITAU_FIC_FIM or DIFERENCIADO_CREDITO_PRIVADO_LONGO_PRAZO_RENDA_FIXA \")\n",
    "\n",
    "\n",
    "llm_structured_output = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "structured_llm = llm_structured_output.with_structured_output(SearchQuery)\n",
    "\n",
    "response = structured_llm.invoke(\"Please, convert the user query: Qual Ã© o hÃ³rÃ¡rio limite do fundo fic fim ? \")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc011732",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7340c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
