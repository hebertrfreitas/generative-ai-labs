{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postgres + PGVECTOR examples (with llama-index framework)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, StorageContext\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.vector_stores.postgres import PGVectorStore\n",
    "import textwrap\n",
    "import openai\n",
    "import os\n",
    "from getpass import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you need to have you openAI key exported like \n",
    "#! export OPENAI_API_KEY = ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "91646.50s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "91651.83s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 75042  100 75042    0     0   278k      0 --:--:-- --:--:-- --:--:--  277k\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p 'data/paul_graham/'\n",
    "!curl -L 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -o 'data/paul_graham/paul_graham_essay.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "353\n"
     ]
    }
   ],
   "source": [
    "with open(\"./data/paul_graham/paul_graham_essay.txt\", 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "\n",
    "lines = content.splitlines()\n",
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodes size: 353\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.schema import Node, TextNode\n",
    "documents = SimpleDirectoryReader(\"./data/paul_graham\").load_data()\n",
    "#print(documents)\n",
    "sentence_splitter = SentenceSplitter(chunk_size=500, chunk_overlap=100)\n",
    "#nodes = sentence_splitter.get_nodes_from_documents(documents)\n",
    "nodes = [TextNode(text=line) for line in lines]\n",
    "print(f'nodes size: {len(nodes)}')\n",
    "# for node in nodes:\n",
    "#     print(node.text) \n",
    "\n",
    "\n",
    "#print(\"Document ID:\", documents[0].doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "connection_string = \"postgresql://postgres:postgres@localhost:5432\"\n",
    "db_name = \"vector_db_llama_index\"\n",
    "conn = psycopg2.connect(connection_string)\n",
    "conn.autocommit = True\n",
    "\n",
    "with conn.cursor() as c:\n",
    "    kill_connection_query  = f\"\"\"\n",
    "                                SELECT \n",
    "                                    pg_terminate_backend(pid) \n",
    "                                FROM \n",
    "                                    pg_stat_activity \n",
    "                                WHERE \n",
    "                                    -- don't kill my own connection!\n",
    "                                    pid <> pg_backend_pid()\n",
    "                                    -- don't kill the connections to other databases\n",
    "                                    AND datname = '{db_name}'\n",
    "                                    ;\n",
    "                            \"\"\"\n",
    "    \n",
    "    c.execute(kill_connection_query)\n",
    "\n",
    "with conn.cursor() as c:\n",
    "    c.execute(f\"DROP DATABASE IF EXISTS {db_name}\")\n",
    "    c.execute(f\"CREATE DATABASE {db_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import make_url\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "\n",
    "embed_model = OllamaEmbedding(\n",
    "    model_name=\"all-minilm\",  # or your preferred Ollama embedding model\n",
    "    ollama_base_url=\"http://localhost:11434\"  # default Ollama endpoint\n",
    ")\n",
    "\n",
    "url = make_url(connection_string)\n",
    "\n",
    "vector_store_hnsw = PGVectorStore.from_params(\n",
    "    database=db_name,\n",
    "    host=url.host,\n",
    "    password=url.password,\n",
    "    port=url.port,\n",
    "    user=url.username,\n",
    "    table_name=\"paul_graham_essay_hnsw\",\n",
    "    #llama 3.1 4096\n",
    "    # all-minilm 284\n",
    "    embed_dim=384, \n",
    "    hnsw_kwargs={\n",
    "        \"hnsw_m\": 16,\n",
    "        \"hnsw_ef_construction\": 64,\n",
    "        \"hnsw_ef_search\": 40,\n",
    "        \"hnsw_dist_method\": \"vector_cosine_ops\",\n",
    "    },    \n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some nodes are missing content, skipping them...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 171/171 [00:27<00:00,  6.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'llama_index.core.query_engine.retriever_query_engine.RetrieverQueryEngine'>\n"
     ]
    }
   ],
   "source": [
    "storage_context_hnsw = StorageContext.from_defaults(vector_store=vector_store_hnsw)\n",
    "index_hnsw = VectorStoreIndex(nodes=nodes, storage_context=storage_context_hnsw, show_progress=True, embed_model=embed_model)\n",
    "# .from_documents(\n",
    "#     documents, storage_context=storage_context, show_progress=True, embed_model=embed_model, \n",
    "# )\n",
    "query_engine_hnsw = index_hnsw.as_query_engine()\n",
    "print(type(query_engine_hnsw))\n",
    "retriever_hnsw = index_hnsw.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Thanks to Trevor Blackwell, John Collison, Patrick Collison, Daniel Gackle, Ralph Hazell, Jessica Livingston, Robert Morris, and Harj Taggar for reading drafts of this.\n",
      "Score: 0.4695280188853659\n",
      "Text: In the print era, the channel for publishing essays had been vanishingly small. Except for a few officially anointed thinkers who went to the right parties in New York, the only people allowed to publish essays were specialists writing about their specialties. There were so many essays that had never been written, because there had been no way to publish them. Now they could be, and I was going to write them. [12]\n",
      "Score: 0.37026984668241836\n"
     ]
    }
   ],
   "source": [
    "retriever_response = retriever_hnsw.retrieve(\"What did the author do ?\",)\n",
    "#print(textwrap.fill(str(retriever_response), 100))\n",
    "for result in retriever_response:\n",
    "    print(\"Text:\", result.node.text)\n",
    "    print(\"Score:\", result.score)\n",
    "\n",
    "\n",
    "# response = query_engine.query(\"What did the author do?\",)\n",
    "# print(textwrap.fill(str(response), 100))\n",
    "# for result in response.source_nodes:\n",
    "#     print(\"Text:\", result.node.text)\n",
    "#     print(\"Score:\", result.score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### resultados anteriores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```log\n",
    "Text: I wanted to go back to RISD, but I was now broke and RISD was very expensive, so I decided to get a job for a year and then return to RISD the next fall. I got one at a company called Interleaf, which made software for creating documents. You mean like Microsoft Word? Exactly. That was how I learned that low end software tends to eat high end software. But Interleaf still had a few years to live yet. [5]\n",
    "Score: 0.5052353369340625\n",
    "Text: The problem with systems work, though, was that it didn't last. Any program you wrote today, no matter how good, would be obsolete in a couple decades at best. People might mention your software in footnotes, but no one would actually use it. And indeed, it would seem very feeble work. Only people with a sense of the history of the field would even realize that, in its time, it had been good.\n",
    "Score: 0.5035586739636687\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
